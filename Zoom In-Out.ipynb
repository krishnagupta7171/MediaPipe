{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08d8db-6a12-4649-ae9b-e4f7d1e71620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math #To calculate distances between fingers\n",
    "\n",
    "# Load image to zoom\n",
    "img_to_zoom = cv2.imread(\"prysh.jpg\") #If this file fails to load then img_to_zoom will be none\n",
    "if img_to_zoom is None:\n",
    "    img_to_zoom = 255 * np.ones((400, 400, 3), dtype=np.uint8)\n",
    "\n",
    "# - If the image wasn't found, it creates a blank white image instead.\n",
    "#- np.ones((400, 400, 3), dtype=np.uint8) generates a 400x400 pixel array with 3 color channels (RGB).\n",
    "#- Multiplying by 255 makes all pixels fully white (255, 255, 255).\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands  #used to detect hand landmarks\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7) #- Hands() initializes the hand tracking model.\n",
    "#min_detection_confidence is a threshold that determines how confident the model must be before it considers a hand detection valid\n",
    "#- The MediaPipe Hands model assigns a confidence score (from 0 to 1) to every detected hand.\n",
    "#- If the score is higher than min_detection_confidence, the model accepts the detection as valid.\n",
    "#- If the score is lower, it ignores that detection to avoid false positives.\n",
    "\n",
    "\n",
    "\n",
    "# Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Zoom settings\n",
    "zoom_scale = 1.0 #Initial zoom scale = 1.0x.\n",
    "min_zoom = 0.2 #- This sets the minimum zoom limit (50% of the original size) \n",
    "max_zoom = 5.0 #- This sets the maximum zoom limit (500% scale).\n",
    "prev_distance = None #- This stores the previous thumb-index distance during hand tracking. If none means no previous gesture\n",
    "gesture_reset_ready = False #- This tracks whether the zoom reset gesture has been detected. When True, the system resets the zoom baseline for fresh tracking.\n",
    "dead_zone = 5 #- This prevents small movements from causing zoom changes. Any movement below 5 pixel is ignored\n",
    "\n",
    "while True:  #Reads a frame from the webcam.If it fails, exits the loop, success is boolean value 0,1 and img contains frames\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    img = cv2.flip(img, 1) #This flips the image horizontally (left to right).\n",
    "    h, w, _ = img.shape #Retrieves the height (h), width (w), and color channels (_) of the image. This is useful for resizing, cropping, and placing objects within the frame.\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb) # Sends the image to the MediaPipe Hands model for hand detection.\n",
    "# If hands are found, results.multi_hand_landmarks will store the detected landmarks (key points like fingers, wrist, etc.).\n",
    "\n",
    "    if results.multi_hand_landmarks and results.multi_handedness: #Checks if any hands were detected and whether \n",
    "        #handedness info is available (left/right).\n",
    "        hand_labels = [hand.classification[0].label for hand in results.multi_handedness] # In simple hand_labels is a list which contains the hands either left or right.\n",
    "        #- This list comprehension creates a list hand_labels where each detected hand is labeled as \"Right\" or \"Left\".\n",
    "        # for better go to chatgpt for explaination\n",
    "        hand_landmarks = results.multi_hand_landmarks #- Stores the hand landmarks (finger joints, wrist positions) for each detected hand.\n",
    "\n",
    "        right_hand_index = None\n",
    "        left_hand_index = None\n",
    "\n",
    "        # Determine hand indices\n",
    "        # ✅ This starts a loop through the hand_labels list, where: - i represents the index (position in the list). - label is the value (\"Right\" or \"Left\"), specifying the hand type.\n",
    "        #Example: If hand_labels = [\"Right\", \"Left\"],- First iteration: i = 0, label = \"Right\"- Second iteration: i = 1, label = \"Left\"\n",
    "\n",
    "        for i, label in enumerate(hand_labels):\n",
    "            if label == \"Right\":\n",
    "                right_hand_index = i\n",
    "            elif label == \"Left\":\n",
    "                left_hand_index = i\n",
    "\n",
    "        # Handle Left Hand (Reset Zoom to 1.0x)\n",
    "        if left_hand_index is not None:  #- This condition checks whether a left hand is detected.\n",
    "            zoom_scale = 1.0\n",
    "            prev_distance = None #- This ensures that zoom resets when the left hand is detected.\n",
    "            cv2.putText(img, \"Left hand detected - Reset Zoom\", (10, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "        # Handle Right Hand (Zoom In/Out)\n",
    "        if right_hand_index is not None: #- Verifies if a right hand was detected\n",
    "            hand = hand_landmarks[right_hand_index] #- Stores the coordinates of finger joints in hand\n",
    "            mp_draw.draw_landmarks(img, hand, mp_hands.HAND_CONNECTIONS) #This function is used to draw hand landmarks on an image.\n",
    "            # & if we add  mp_hands.HAND_CONNECTIONS in parameter then it will connect the dots using lines\n",
    "\n",
    "\n",
    "            # Thumb & Index tip for zoom\n",
    "            x_thumb = int(hand.landmark[4].x * w) #Extracts the thumb tip coordinates (landmark[4]). hand.landmark[4].x and hand.landmark[4].y provide normalized values (0 to 1).\n",
    "            y_thumb = int(hand.landmark[4].y * h) #Multiplies by w (image width) and h (image height) to convert to pixel coordinates.\n",
    "            x_index = int(hand.landmark[8].x * w) # Extracts the index finger tip coordinates (landmark[8]).\n",
    "            y_index = int(hand.landmark[8].y * h) # Converts normalized values to pixel coordinates, same as above.\n",
    "            x_middle_tip = int(hand.landmark[12].x * w) #Extracts the middle finger tip coordinates (landmark[12]).\n",
    "            y_middle_tip = int(hand.landmark[12].y * h) #Used for additional gesture recognition, like multi-finger zoom control.\n",
    "            y_middle_pip = int(hand.landmark[10].y * h) #Extracts the middle finger joint (PIP - Proximal Interphalangeal Joint) (landmark[10]).\n",
    "            #✅ Used to determine finger bending or pinch gestures.\n",
    "            # Draw\n",
    "            cv2.circle(img, (x_thumb, y_thumb), 8, (255, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x_index, y_index), 8, (255, 0, 255), cv2.FILLED)\n",
    "            # cv2.line(img, (x_thumb, y_thumb), (x_index, y_index), (0, 255, 0), 2)\n",
    "\n",
    "            # Measure pinch distance\n",
    "            distance = math.hypot(x_index - x_thumb, y_index - y_thumb) # This will calculate the distance which helps in zoom in/out.\n",
    "            # eg- When you bring your thumb and index finger closer, it reduces the distance, triggering a zoom-out action\n",
    "            # Gesture reset signal with middle finger\n",
    "            if y_middle_tip < y_middle_pip: #Detects if the middle finger tip is positioned above the middle finger joint (PIP). condition for zoom reset\n",
    "                gesture_reset_ready = True #Marks the reset gesture as \"ready\" \n",
    "                prev_distance = None #Clears previous distance to prevent incorrect zoom adjustments.\n",
    "                cv2.putText(img, \"Reset gesture detected\", (10, 140),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            elif gesture_reset_ready: #Checks if reset gesture was performed earlier\n",
    "                #  Allows zoom modifications only after a reset gesture\n",
    "                # Apply zoom after reset gesture\n",
    "                if prev_distance is not None: #Ensures prev_distance isn't empty before making zoom adjustments.\n",
    "                    diff = distance - prev_distance #Helps determine whether fingers moved apart (zoom in) or together (zoom out).Used for ignoring small distance.\n",
    "                    if abs(diff) > dead_zone: # Ignores small finger movements below a threshold (dead_zone).\n",
    "                        zoom_change = diff * 0.005 # Calculates zoom factor (zoom_change) based on finger movement (diff)\n",
    "                        #0.005 scales the change for smooth adjustments\n",
    "                        zoom_scale += zoom_change #Updates zoom level \n",
    "                        zoom_scale = max(min_zoom, min(max_zoom, zoom_scale))# Ensures zoom scale remains between min_zoom and max_zoom limits.\n",
    "                        #Prevents extreme zoom-in or zoom-out.\n",
    "                prev_distance = distance #Stores current distance as prev_distance, ensuring smooth zoom tracking over frames.\n",
    "\n",
    "    else:\n",
    "        prev_distance = None #If no hand is detected, it resets prev_distance to None\n",
    "\n",
    "    # Apply zoom to the image\n",
    "    zoomed_img = cv2.resize(img_to_zoom, None, fx=zoom_scale, fy=zoom_scale) #Resizes the image using OpenCV (cv2.resize) based on the zoom scale (zoom_scale).\n",
    "    # fx and fy are scaling factors for width and height, respectively.\n",
    "    # None means the new dimensions are automatically determined based on scaling.\n",
    "\n",
    "\n",
    "    zh, zw = zoomed_img.shape[:2]\n",
    "    #Extracts the height (zh) and width (zw) of the resized (zoomed_img) image.\n",
    "    # shape[:2] ensures we only get height and width, ignoring color channels.\n",
    "\n",
    "    # Determines how much to crop to keep the image centered after zooming.\\.\n",
    "    # max(0, value) ensures no negative cropping values (avoiding errors).\n",
    "\n",
    "    crop_x = max(0, (zw - w) // 2) #(zw - w) // 2 finds the amount to trim horizontally, ensuring focus\n",
    "    crop_y = max(0, (zh - h) // 2) # (zh - h) // 2 finds the amount to trim vertically, keeping the zoom effect centered\n",
    "    zoomed_crop = zoomed_img[crop_y:crop_y + h, crop_x:crop_x + w] #This extracts a cropped section from the zoomed_img to ensure the displayed image size remains the same after zooming.\n",
    "\n",
    "\n",
    "    # Display\n",
    "    cv2.putText(img, f\"Zoom: {zoom_scale:.2f}x\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 0, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Webcam\", img)\n",
    "    cv2.imshow(\"Zoom Image\", zoomed_crop)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "788e7307-10bc-46f3-b047-c126c6289e7d",
   "metadata": {},
   "source": [
    "When resizing an image with OpenCV, the parameters fx (scale factor for width) and fy (scale factor for height) determine how much the image grows or shrinks compared to its original size.\n",
    "\n",
    "[ \\text{Scaling Change} = ( \\text{Scale Factor} - 1 ) \\times 100% ] ✅ For zoom_scale = 1.2 →\n",
    "[ (1.2 - 1) \\times 100 = 20% ] ✅ For zoom_scale = 0.8 →\n",
    "[ (0.8 - 1) \\times 100 = -20% ] 📌 Positive means scale-up, negative means shrink.\n",
    "\n",
    "\n",
    "\n",
    "zoomed_img[crop_y:crop_y + h, crop_x:crop_x + w]\n",
    "- This selects a region of interest (ROI) from zoomed_img.\n",
    "- Crops the zoomed image from crop_x, crop_y to crop_x + w, crop_y + h.\n",
    "- Ensures the final output maintains the original dimensions (h, w)\n",
    "\n",
    "\n",
    " crop_y:crop_y + h →\n",
    "- Extracts rows (height) from crop_y to crop_y + h.\n",
    "- Keeps the vertical dimension aligned with the original image height.\n",
    "3️⃣ crop_x:crop_x + w →\n",
    "- Extracts columns (width) from crop_x to crop_x + w.\n",
    "- Keeps the horizontal dimension aligned with the original image width.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138da64-c9b1-42ec-a163-f7bc8f7eb20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
