{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36929c6e-20bb-4c27-a031-debbdb101679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cec38a-16b3-4b78-b0a8-d342efc06961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MediaPipe works!\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "print(\"‚úÖ MediaPipe works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fda4d5-71d6-43cd-ba95-97a44af123a2",
   "metadata": {},
   "source": [
    "# Stage 1: Basic Info and Basic Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da221d7e-a9f6-459b-8700-ab6e885bb594",
   "metadata": {},
   "source": [
    "# What is Mediapipe?\n",
    "MediaPipe is an open-source framework by Google for building real-time machine learning pipelines ‚Äî especially for computer vision tasks.\n",
    "\n",
    "In simple words: MediaPipe helps you detect faces, hands, pose, objects, gestures, etc. from videos or images ‚Äî using pretrained ML models, in real-time, and very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702889b8-a000-440a-9b22-13e032248e44",
   "metadata": {},
   "source": [
    "# Why mediapipe?\n",
    "‚úÖ Feature --------------------------------------------------------- üí¨ What It Means\n",
    "\n",
    "Pre-trained Models ------------------------------------------------ No need to train ‚Äî just use and go!\n",
    "\n",
    "Real-time Processing ---------------------------------------------- Detect gestures, poses instantly on webcam\n",
    "\n",
    "Cross-Platform ------------------------------------------------ Works on Python, Android, iOS, C++, Web\n",
    "\n",
    "Lightweight ------------------------------------------------------ Can run on low-end devices like phones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea02109-6969-4f95-ae30-c028b8e62772",
   "metadata": {},
   "source": [
    "# What Mediapipe do?\n",
    "\n",
    "üß© Module-------------------------------üí° What It Does\n",
    "\n",
    "Hands--------------------------------------Detects 21 hand landmarks per hand ü§ö\n",
    "\n",
    "Face Mesh-------------------------------- Detects 468 facial landmarks üòØ\n",
    "\n",
    "Pose------------------------------------ Full-body pose detection üßç‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Holistic-------------------------------- Combines Hands + Face + Pose üß†\n",
    "\n",
    "Selfie Segmentation ----------------------------Separates person from background ‚úÇÔ∏è\n",
    "\n",
    "Objectron------------------------------------ Detects 3D objects like shoes, cups üõçÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f1a4e-10c4-42cb-a988-fc296c30edc3",
   "metadata": {},
   "source": [
    "# How It Works\n",
    "Takes input from webcam or video\n",
    "\n",
    "Passes it through ML models\n",
    "\n",
    "Gives you back keypoints/landmarks\n",
    "\n",
    "we can use these landmarks to build cool apps like:\n",
    "\n",
    " 1. Gesture controls\n",
    " 2. Pose-based games\n",
    " 3. Virtual makeup\n",
    " 4. Air drawing \n",
    " 5. Sign language recognition\n",
    " 6. Fitness posture checkers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a841d32-f7d7-435f-8e52-155e74973239",
   "metadata": {},
   "source": [
    "# Task 1: Real-time Hand Landmark Detection\n",
    "üßæ What This Will Do:\n",
    "\n",
    "Open the webcam üì∏\n",
    "\n",
    "Detect our hands ‚úã\n",
    "\n",
    "Draw 21 landmark points + hand connections in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52d6168-6ad3-43ed-80ec-3415eff7284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)        #Opens the default webcam (0) for capturing live video frames\n",
    "\n",
    "hands = mp.solutions.hands.Hands()\n",
    "draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:         #Keeps running frame-by-frame until you press 'q' to quit.\n",
    "    ret, frame = cap.read() #cap.read() captures one frame from webcam\n",
    "                           # If it fails (ret == False), we stop the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #OpenCV captures in BGR, but MediaPipe needs RGB\n",
    "                                        #This converts the color format\n",
    "    result = hands.process(img_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand in result.multi_hand_landmarks:\n",
    "            draw.draw_landmarks(frame, hand, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "    frame = cv2.flip(frame,1)  # This will flip the video since there is a change in left and right direction \n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):     #Checks if you pressed the p keyvIf yes, breaks the loop\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563dbd3-60a7-473e-b587-5f1ae9f7f141",
   "metadata": {},
   "source": [
    "mp.solutions is just a namespace inside the MediaPipe package.\n",
    "\n",
    "It groups all the different ML models: hands, face_mesh, pose, holistic, etc.\n",
    "\n",
    "When you do mp.solutions.hands, you're accessing the Hands module\n",
    "\n",
    "mp.solutions is not just a module full of functions ‚Äî it's more like a package (submodule) inside MediaPipe that contains different ML models as classes, not just functions.\n",
    "\n",
    "mediapipe/\n",
    "\n",
    "‚îÇ\n",
    "\n",
    "‚îú‚îÄ‚îÄ solutions/\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ hands.py ‚Üê Hand tracking model\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ pose.py ‚Üê Full-body pose model\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ face_mesh.py ‚Üê Facial landmark model\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ holistic.py ‚Üê Combo of all above\n",
    "\n",
    "‚îÇ ‚îî‚îÄ‚îÄ drawing_utils.py‚Üê Functions for drawing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42acfc-de5b-40b5-81cd-ab2a9ccab801",
   "metadata": {},
   "source": [
    "mp.solutions.hands: Loads the hand tracking module\n",
    "\n",
    "hands = mp_hands.Hands(): Initializes the model with default settings (detects up to 2 hands, in real-time)\n",
    "\n",
    "mp_draw: A helper to draw landmarks on the hand points and connections between these points (like skeleton lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843383b2-3b6d-4084-81be-1964cbce3541",
   "metadata": {},
   "source": [
    "# results = hands.process(img_rgb)\n",
    "This line is where MediaPipe does its job.\n",
    "\n",
    "üîç Step-by-step Explanation:\n",
    "\n",
    "img_rgb:\n",
    "\n",
    "This is the current image frame (from your webcam, for example).\n",
    "\n",
    "It has been converted to RGB format because MediaPipe works with RGB images, not BGR (which OpenCV uses by default).\n",
    "\n",
    "-hands.process(img_rgb):\n",
    "\n",
    "MediaPipe looks at the image and tries to find hands in it.\n",
    "\n",
    "It uses a machine learning model (a pre-trained deep learning model) to do this.\n",
    "\n",
    "It analyzes the image and checks: \"Are there any hands here? If yes, where exactly?\"\n",
    "\n",
    "-results:\n",
    "\n",
    "The output of the .process() method.\n",
    "\n",
    "It contains information about what was detected in the image.\n",
    "\n",
    "üñêÔ∏è results.multi_hand_landmarks\n",
    "\n",
    "If MediaPipe finds hands, this will be a list of hands.\n",
    "\n",
    "Each hand has 21 landmarks (important points like fingertips, joints, etc.).\n",
    "\n",
    "You can loop through this list to draw or analyze hand positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee8d7f-64b4-439a-902a-03892b9aae91",
   "metadata": {},
   "source": [
    "# Loop through the 21 landmarks\n",
    "    for id, lm in enumerate(hand_landmarks.landmark):\n",
    "        # lm contains x, y, z (normalized values between 0 and 1)\n",
    "        h, w, c = img.shape  # Get image height, width, channels\n",
    "        cx, cy = int(lm.x * w), int(lm.y * h)  # Convert to pixel values\n",
    "\n",
    "        print(f\"Landmark {id}: (x={cx}, y={cy})\")\n",
    "\n",
    "\n",
    "\n",
    "        This code will give the co ordinates of all those 21 points in our hand\n",
    "\n",
    "        A breakdown of code:\n",
    "\n",
    "        hand_landmarks.landmark ‚Üí List of 21 points on the hand.\n",
    "\n",
    "        enumerate(...) ‚Üí Gives you both the landmark index (id) and the landmark itself (lm).\n",
    "\n",
    "        lm.x and lm.y ‚Üí Coordinates in normalized form (0 to 1).\n",
    "\n",
    "        cx, cy ‚Üí Converted to pixel positions using image width and height.\n",
    "\n",
    "        print(...) ‚Üí Shows the (x, y) location of each point in the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a582bf-d0f3-4640-996b-a18f3c2b13e1",
   "metadata": {},
   "source": [
    "# Stage 2 :\n",
    "# Task 1: Extract Hand Landmark Coordinates"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89b058a8-6391-4704-a88f-47b982627141",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "üëâ Detect hand landmarks like before\n",
    "\n",
    "üëâ Extract the x, y, z values of each landmark\n",
    "\n",
    "üëâ Print them in the console"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a908c4e4-1a11-42e2-a71b-98b93f3fbdb8",
   "metadata": {},
   "source": [
    "Concept:\n",
    "\n",
    "MediaPipe‚Äôs hand model gives 21 landmarks per hand.\n",
    "\n",
    "Each landmark has:\n",
    "\n",
    "x: horizontal position (normalized between 0 and 1)\n",
    "\n",
    "y: vertical position (normalized between 0 and 1)\n",
    "\n",
    "z: relative depth (into/out of the screen)\n",
    "\n",
    "We can multiply x and y by the frame width and height to get pixel positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "896e498d-9dc7-45c8-bbb7-95aa4ab3d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmark 0: X=79, Y=327\n",
      "Landmark 1: X=122, Y=322\n",
      "Landmark 2: X=164, Y=301\n",
      "Landmark 3: X=197, Y=280\n",
      "Landmark 4: X=221, Y=259\n",
      "Landmark 5: X=142, Y=239\n",
      "Landmark 6: X=163, Y=199\n",
      "Landmark 7: X=174, Y=172\n",
      "Landmark 8: X=183, Y=148\n",
      "Landmark 9: X=116, Y=225\n",
      "Landmark 10: X=132, Y=178\n",
      "Landmark 11: X=142, Y=146\n",
      "Landmark 12: X=149, Y=120\n",
      "Landmark 13: X=89, Y=223\n",
      "Landmark 14: X=100, Y=177\n",
      "Landmark 15: X=108, Y=147\n",
      "Landmark 16: X=115, Y=123\n",
      "Landmark 17: X=61, Y=230\n",
      "Landmark 18: X=63, Y=195\n",
      "Landmark 19: X=66, Y=170\n",
      "Landmark 20: X=70, Y=149\n",
      "Landmark 0: X=83, Y=320\n",
      "Landmark 1: X=129, Y=312\n",
      "Landmark 2: X=172, Y=291\n",
      "Landmark 3: X=202, Y=268\n",
      "Landmark 4: X=226, Y=248\n",
      "Landmark 5: X=149, Y=223\n",
      "Landmark 6: X=170, Y=181\n",
      "Landmark 7: X=182, Y=155\n",
      "Landmark 8: X=191, Y=132\n",
      "Landmark 9: X=122, Y=211\n",
      "Landmark 10: X=138, Y=162\n",
      "Landmark 11: X=146, Y=130\n",
      "Landmark 12: X=153, Y=104\n",
      "Landmark 13: X=95, Y=210\n",
      "Landmark 14: X=108, Y=162\n",
      "Landmark 15: X=115, Y=132\n",
      "Landmark 16: X=122, Y=108\n",
      "Landmark 17: X=67, Y=218\n",
      "Landmark 18: X=69, Y=182\n",
      "Landmark 19: X=71, Y=158\n",
      "Landmark 20: X=76, Y=138\n",
      "Landmark 0: X=80, Y=324\n",
      "Landmark 1: X=129, Y=314\n",
      "Landmark 2: X=172, Y=293\n",
      "Landmark 3: X=203, Y=269\n",
      "Landmark 4: X=227, Y=249\n",
      "Landmark 5: X=148, Y=222\n",
      "Landmark 6: X=170, Y=181\n",
      "Landmark 7: X=182, Y=155\n",
      "Landmark 8: X=191, Y=131\n",
      "Landmark 9: X=121, Y=210\n",
      "Landmark 10: X=138, Y=160\n",
      "Landmark 11: X=146, Y=129\n",
      "Landmark 12: X=153, Y=103\n",
      "Landmark 13: X=95, Y=209\n",
      "Landmark 14: X=106, Y=162\n",
      "Landmark 15: X=115, Y=133\n",
      "Landmark 16: X=122, Y=108\n",
      "Landmark 17: X=67, Y=217\n",
      "Landmark 18: X=68, Y=182\n",
      "Landmark 19: X=71, Y=158\n",
      "Landmark 20: X=75, Y=136\n",
      "Landmark 0: X=73, Y=324\n",
      "Landmark 1: X=122, Y=315\n",
      "Landmark 2: X=165, Y=291\n",
      "Landmark 3: X=196, Y=266\n",
      "Landmark 4: X=219, Y=245\n",
      "Landmark 5: X=142, Y=220\n",
      "Landmark 6: X=164, Y=177\n",
      "Landmark 7: X=175, Y=149\n",
      "Landmark 8: X=183, Y=124\n",
      "Landmark 9: X=115, Y=207\n",
      "Landmark 10: X=131, Y=156\n",
      "Landmark 11: X=140, Y=123\n",
      "Landmark 12: X=146, Y=95\n",
      "Landmark 13: X=88, Y=206\n",
      "Landmark 14: X=100, Y=157\n",
      "Landmark 15: X=107, Y=127\n",
      "Landmark 16: X=114, Y=100\n",
      "Landmark 17: X=59, Y=215\n",
      "Landmark 18: X=58, Y=179\n",
      "Landmark 19: X=60, Y=155\n",
      "Landmark 20: X=64, Y=133\n",
      "Landmark 0: X=68, Y=327\n",
      "Landmark 1: X=118, Y=315\n",
      "Landmark 2: X=162, Y=292\n",
      "Landmark 3: X=192, Y=266\n",
      "Landmark 4: X=213, Y=243\n",
      "Landmark 5: X=137, Y=218\n",
      "Landmark 6: X=158, Y=174\n",
      "Landmark 7: X=168, Y=146\n",
      "Landmark 8: X=177, Y=120\n",
      "Landmark 9: X=109, Y=207\n",
      "Landmark 10: X=124, Y=154\n",
      "Landmark 11: X=132, Y=120\n",
      "Landmark 12: X=138, Y=91\n",
      "Landmark 13: X=82, Y=207\n",
      "Landmark 14: X=93, Y=157\n",
      "Landmark 15: X=100, Y=125\n",
      "Landmark 16: X=105, Y=98\n",
      "Landmark 17: X=52, Y=217\n",
      "Landmark 18: X=52, Y=180\n",
      "Landmark 19: X=53, Y=155\n",
      "Landmark 20: X=55, Y=131\n",
      "Landmark 0: X=68, Y=326\n",
      "Landmark 1: X=118, Y=315\n",
      "Landmark 2: X=162, Y=292\n",
      "Landmark 3: X=193, Y=266\n",
      "Landmark 4: X=215, Y=243\n",
      "Landmark 5: X=136, Y=218\n",
      "Landmark 6: X=158, Y=174\n",
      "Landmark 7: X=169, Y=145\n",
      "Landmark 8: X=177, Y=120\n",
      "Landmark 9: X=109, Y=206\n",
      "Landmark 10: X=124, Y=153\n",
      "Landmark 11: X=131, Y=119\n",
      "Landmark 12: X=137, Y=91\n",
      "Landmark 13: X=82, Y=206\n",
      "Landmark 14: X=93, Y=156\n",
      "Landmark 15: X=99, Y=125\n",
      "Landmark 16: X=105, Y=98\n",
      "Landmark 17: X=54, Y=216\n",
      "Landmark 18: X=52, Y=180\n",
      "Landmark 19: X=52, Y=154\n",
      "Landmark 20: X=54, Y=131\n",
      "Landmark 0: X=69, Y=328\n",
      "Landmark 1: X=117, Y=316\n",
      "Landmark 2: X=161, Y=291\n",
      "Landmark 3: X=192, Y=264\n",
      "Landmark 4: X=212, Y=240\n",
      "Landmark 5: X=136, Y=218\n",
      "Landmark 6: X=156, Y=173\n",
      "Landmark 7: X=166, Y=144\n",
      "Landmark 8: X=174, Y=117\n",
      "Landmark 9: X=108, Y=206\n",
      "Landmark 10: X=122, Y=152\n",
      "Landmark 11: X=129, Y=117\n",
      "Landmark 12: X=134, Y=88\n",
      "Landmark 13: X=80, Y=206\n",
      "Landmark 14: X=90, Y=156\n",
      "Landmark 15: X=96, Y=123\n",
      "Landmark 16: X=101, Y=94\n",
      "Landmark 17: X=50, Y=217\n",
      "Landmark 18: X=49, Y=179\n",
      "Landmark 19: X=49, Y=153\n",
      "Landmark 20: X=51, Y=130\n",
      "Landmark 0: X=69, Y=328\n",
      "Landmark 1: X=118, Y=316\n",
      "Landmark 2: X=161, Y=291\n",
      "Landmark 3: X=192, Y=264\n",
      "Landmark 4: X=213, Y=241\n",
      "Landmark 5: X=136, Y=218\n",
      "Landmark 6: X=156, Y=172\n",
      "Landmark 7: X=166, Y=143\n",
      "Landmark 8: X=175, Y=117\n",
      "Landmark 9: X=108, Y=206\n",
      "Landmark 10: X=121, Y=153\n",
      "Landmark 11: X=128, Y=117\n",
      "Landmark 12: X=133, Y=87\n",
      "Landmark 13: X=80, Y=207\n",
      "Landmark 14: X=89, Y=156\n",
      "Landmark 15: X=96, Y=123\n",
      "Landmark 16: X=102, Y=95\n",
      "Landmark 17: X=51, Y=217\n",
      "Landmark 18: X=48, Y=179\n",
      "Landmark 19: X=48, Y=153\n",
      "Landmark 20: X=51, Y=129\n",
      "Landmark 0: X=73, Y=330\n",
      "Landmark 1: X=121, Y=317\n",
      "Landmark 2: X=165, Y=289\n",
      "Landmark 3: X=196, Y=262\n",
      "Landmark 4: X=219, Y=241\n",
      "Landmark 5: X=137, Y=217\n",
      "Landmark 6: X=159, Y=172\n",
      "Landmark 7: X=170, Y=142\n",
      "Landmark 8: X=178, Y=116\n",
      "Landmark 9: X=110, Y=205\n",
      "Landmark 10: X=124, Y=150\n",
      "Landmark 11: X=131, Y=115\n",
      "Landmark 12: X=135, Y=86\n",
      "Landmark 13: X=83, Y=205\n",
      "Landmark 14: X=92, Y=154\n",
      "Landmark 15: X=98, Y=122\n",
      "Landmark 16: X=103, Y=95\n",
      "Landmark 17: X=55, Y=216\n",
      "Landmark 18: X=52, Y=177\n",
      "Landmark 19: X=53, Y=151\n",
      "Landmark 20: X=55, Y=129\n",
      "Landmark 0: X=73, Y=330\n",
      "Landmark 1: X=121, Y=316\n",
      "Landmark 2: X=166, Y=289\n",
      "Landmark 3: X=198, Y=263\n",
      "Landmark 4: X=220, Y=240\n",
      "Landmark 5: X=138, Y=216\n",
      "Landmark 6: X=160, Y=171\n",
      "Landmark 7: X=170, Y=142\n",
      "Landmark 8: X=179, Y=117\n",
      "Landmark 9: X=111, Y=205\n",
      "Landmark 10: X=124, Y=150\n",
      "Landmark 11: X=131, Y=115\n",
      "Landmark 12: X=136, Y=86\n",
      "Landmark 13: X=83, Y=205\n",
      "Landmark 14: X=92, Y=153\n",
      "Landmark 15: X=98, Y=121\n",
      "Landmark 16: X=104, Y=95\n",
      "Landmark 17: X=55, Y=215\n",
      "Landmark 18: X=53, Y=177\n",
      "Landmark 19: X=52, Y=152\n",
      "Landmark 20: X=54, Y=129\n",
      "Landmark 0: X=103, Y=343\n",
      "Landmark 1: X=151, Y=330\n",
      "Landmark 2: X=196, Y=302\n",
      "Landmark 3: X=223, Y=273\n",
      "Landmark 4: X=247, Y=250\n",
      "Landmark 5: X=170, Y=229\n",
      "Landmark 6: X=192, Y=185\n",
      "Landmark 7: X=203, Y=158\n",
      "Landmark 8: X=211, Y=135\n",
      "Landmark 9: X=144, Y=217\n",
      "Landmark 10: X=158, Y=168\n",
      "Landmark 11: X=168, Y=137\n",
      "Landmark 12: X=175, Y=112\n",
      "Landmark 13: X=118, Y=214\n",
      "Landmark 14: X=130, Y=167\n",
      "Landmark 15: X=139, Y=139\n",
      "Landmark 16: X=148, Y=117\n",
      "Landmark 17: X=93, Y=219\n",
      "Landmark 18: X=94, Y=182\n",
      "Landmark 19: X=97, Y=159\n",
      "Landmark 20: X=100, Y=139\n",
      "Landmark 0: X=121, Y=357\n",
      "Landmark 1: X=170, Y=348\n",
      "Landmark 2: X=212, Y=319\n",
      "Landmark 3: X=240, Y=294\n",
      "Landmark 4: X=260, Y=275\n",
      "Landmark 5: X=203, Y=249\n",
      "Landmark 6: X=222, Y=209\n",
      "Landmark 7: X=234, Y=187\n",
      "Landmark 8: X=244, Y=167\n",
      "Landmark 9: X=182, Y=238\n",
      "Landmark 10: X=203, Y=195\n",
      "Landmark 11: X=216, Y=171\n",
      "Landmark 12: X=228, Y=151\n",
      "Landmark 13: X=160, Y=234\n",
      "Landmark 14: X=178, Y=193\n",
      "Landmark 15: X=193, Y=171\n",
      "Landmark 16: X=206, Y=154\n",
      "Landmark 17: X=134, Y=236\n",
      "Landmark 18: X=147, Y=200\n",
      "Landmark 19: X=159, Y=185\n",
      "Landmark 20: X=171, Y=174\n",
      "Landmark 0: X=123, Y=355\n",
      "Landmark 1: X=171, Y=345\n",
      "Landmark 2: X=215, Y=319\n",
      "Landmark 3: X=242, Y=295\n",
      "Landmark 4: X=262, Y=275\n",
      "Landmark 5: X=203, Y=251\n",
      "Landmark 6: X=222, Y=212\n",
      "Landmark 7: X=233, Y=188\n",
      "Landmark 8: X=244, Y=169\n",
      "Landmark 9: X=182, Y=240\n",
      "Landmark 10: X=202, Y=197\n",
      "Landmark 11: X=214, Y=171\n",
      "Landmark 12: X=226, Y=150\n",
      "Landmark 13: X=160, Y=236\n",
      "Landmark 14: X=178, Y=196\n",
      "Landmark 15: X=191, Y=174\n",
      "Landmark 16: X=204, Y=157\n",
      "Landmark 17: X=134, Y=239\n",
      "Landmark 18: X=146, Y=205\n",
      "Landmark 19: X=157, Y=188\n",
      "Landmark 20: X=171, Y=176\n",
      "Landmark 0: X=129, Y=376\n",
      "Landmark 1: X=178, Y=378\n",
      "Landmark 2: X=226, Y=367\n",
      "Landmark 3: X=257, Y=345\n",
      "Landmark 4: X=280, Y=322\n",
      "Landmark 5: X=235, Y=303\n",
      "Landmark 6: X=267, Y=278\n",
      "Landmark 7: X=284, Y=268\n",
      "Landmark 8: X=294, Y=264\n",
      "Landmark 9: X=216, Y=281\n",
      "Landmark 10: X=249, Y=247\n",
      "Landmark 11: X=266, Y=234\n",
      "Landmark 12: X=275, Y=228\n",
      "Landmark 13: X=192, Y=269\n",
      "Landmark 14: X=222, Y=233\n",
      "Landmark 15: X=237, Y=224\n",
      "Landmark 16: X=245, Y=223\n",
      "Landmark 17: X=160, Y=264\n",
      "Landmark 18: X=179, Y=229\n",
      "Landmark 19: X=193, Y=227\n",
      "Landmark 20: X=201, Y=236\n",
      "Landmark 0: X=411, Y=353\n",
      "Landmark 1: X=362, Y=361\n",
      "Landmark 2: X=307, Y=340\n",
      "Landmark 3: X=265, Y=313\n",
      "Landmark 4: X=228, Y=293\n",
      "Landmark 5: X=341, Y=279\n",
      "Landmark 6: X=300, Y=234\n",
      "Landmark 7: X=270, Y=218\n",
      "Landmark 8: X=250, Y=214\n",
      "Landmark 9: X=368, Y=260\n",
      "Landmark 10: X=324, Y=216\n",
      "Landmark 11: X=293, Y=204\n",
      "Landmark 12: X=273, Y=203\n",
      "Landmark 13: X=390, Y=249\n",
      "Landmark 14: X=352, Y=206\n",
      "Landmark 15: X=322, Y=199\n",
      "Landmark 16: X=303, Y=199\n",
      "Landmark 17: X=407, Y=242\n",
      "Landmark 18: X=375, Y=206\n",
      "Landmark 19: X=349, Y=199\n",
      "Landmark 20: X=331, Y=200\n",
      "Landmark 0: X=412, Y=357\n",
      "Landmark 1: X=363, Y=364\n",
      "Landmark 2: X=307, Y=341\n",
      "Landmark 3: X=265, Y=313\n",
      "Landmark 4: X=231, Y=295\n",
      "Landmark 5: X=338, Y=274\n",
      "Landmark 6: X=311, Y=234\n",
      "Landmark 7: X=292, Y=214\n",
      "Landmark 8: X=278, Y=201\n",
      "Landmark 9: X=363, Y=256\n",
      "Landmark 10: X=334, Y=218\n",
      "Landmark 11: X=313, Y=202\n",
      "Landmark 12: X=299, Y=193\n",
      "Landmark 13: X=386, Y=246\n",
      "Landmark 14: X=361, Y=210\n",
      "Landmark 15: X=339, Y=196\n",
      "Landmark 16: X=323, Y=186\n",
      "Landmark 17: X=408, Y=241\n",
      "Landmark 18: X=389, Y=209\n",
      "Landmark 19: X=373, Y=195\n",
      "Landmark 20: X=359, Y=185\n",
      "Landmark 0: X=389, Y=349\n",
      "Landmark 1: X=335, Y=350\n",
      "Landmark 2: X=275, Y=329\n",
      "Landmark 3: X=233, Y=309\n",
      "Landmark 4: X=199, Y=296\n",
      "Landmark 5: X=296, Y=269\n",
      "Landmark 6: X=270, Y=236\n",
      "Landmark 7: X=257, Y=219\n",
      "Landmark 8: X=248, Y=206\n",
      "Landmark 9: X=319, Y=251\n",
      "Landmark 10: X=297, Y=219\n",
      "Landmark 11: X=283, Y=204\n",
      "Landmark 12: X=275, Y=193\n",
      "Landmark 13: X=340, Y=242\n",
      "Landmark 14: X=323, Y=210\n",
      "Landmark 15: X=309, Y=195\n",
      "Landmark 16: X=299, Y=185\n",
      "Landmark 17: X=360, Y=238\n",
      "Landmark 18: X=343, Y=209\n",
      "Landmark 19: X=329, Y=196\n",
      "Landmark 20: X=318, Y=186\n",
      "Landmark 0: X=373, Y=341\n",
      "Landmark 1: X=325, Y=345\n",
      "Landmark 2: X=267, Y=327\n",
      "Landmark 3: X=226, Y=308\n",
      "Landmark 4: X=193, Y=299\n",
      "Landmark 5: X=283, Y=261\n",
      "Landmark 6: X=260, Y=224\n",
      "Landmark 7: X=249, Y=218\n",
      "Landmark 8: X=242, Y=220\n",
      "Landmark 9: X=308, Y=244\n",
      "Landmark 10: X=286, Y=211\n",
      "Landmark 11: X=275, Y=209\n",
      "Landmark 12: X=271, Y=214\n",
      "Landmark 13: X=331, Y=235\n",
      "Landmark 14: X=312, Y=200\n",
      "Landmark 15: X=299, Y=198\n",
      "Landmark 16: X=293, Y=202\n",
      "Landmark 17: X=353, Y=231\n",
      "Landmark 18: X=333, Y=200\n",
      "Landmark 19: X=320, Y=196\n",
      "Landmark 20: X=314, Y=197\n",
      "Landmark 0: X=326, Y=341\n",
      "Landmark 1: X=267, Y=346\n",
      "Landmark 2: X=220, Y=334\n",
      "Landmark 3: X=185, Y=326\n",
      "Landmark 4: X=157, Y=324\n",
      "Landmark 5: X=218, Y=259\n",
      "Landmark 6: X=188, Y=231\n",
      "Landmark 7: X=166, Y=212\n",
      "Landmark 8: X=146, Y=194\n",
      "Landmark 9: X=236, Y=238\n",
      "Landmark 10: X=217, Y=203\n",
      "Landmark 11: X=206, Y=182\n",
      "Landmark 12: X=199, Y=168\n",
      "Landmark 13: X=257, Y=225\n",
      "Landmark 14: X=240, Y=189\n",
      "Landmark 15: X=221, Y=162\n",
      "Landmark 16: X=204, Y=140\n",
      "Landmark 17: X=281, Y=218\n",
      "Landmark 18: X=264, Y=185\n",
      "Landmark 19: X=247, Y=164\n",
      "Landmark 20: X=231, Y=147\n",
      "Landmark 0: X=314, Y=330\n",
      "Landmark 1: X=266, Y=339\n",
      "Landmark 2: X=217, Y=327\n",
      "Landmark 3: X=184, Y=318\n",
      "Landmark 4: X=158, Y=315\n",
      "Landmark 5: X=210, Y=253\n",
      "Landmark 6: X=179, Y=225\n",
      "Landmark 7: X=157, Y=211\n",
      "Landmark 8: X=140, Y=203\n",
      "Landmark 9: X=228, Y=233\n",
      "Landmark 10: X=199, Y=194\n",
      "Landmark 11: X=179, Y=172\n",
      "Landmark 12: X=162, Y=157\n",
      "Landmark 13: X=249, Y=220\n",
      "Landmark 14: X=224, Y=179\n",
      "Landmark 15: X=203, Y=158\n",
      "Landmark 16: X=185, Y=142\n",
      "Landmark 17: X=274, Y=213\n",
      "Landmark 18: X=255, Y=177\n",
      "Landmark 19: X=236, Y=158\n",
      "Landmark 20: X=217, Y=142\n",
      "Landmark 0: X=316, Y=331\n",
      "Landmark 1: X=269, Y=340\n",
      "Landmark 2: X=218, Y=328\n",
      "Landmark 3: X=183, Y=318\n",
      "Landmark 4: X=157, Y=315\n",
      "Landmark 5: X=212, Y=254\n",
      "Landmark 6: X=181, Y=226\n",
      "Landmark 7: X=161, Y=214\n",
      "Landmark 8: X=145, Y=207\n",
      "Landmark 9: X=228, Y=234\n",
      "Landmark 10: X=197, Y=198\n",
      "Landmark 11: X=175, Y=179\n",
      "Landmark 12: X=156, Y=166\n",
      "Landmark 13: X=249, Y=220\n",
      "Landmark 14: X=223, Y=181\n",
      "Landmark 15: X=201, Y=161\n",
      "Landmark 16: X=182, Y=146\n",
      "Landmark 17: X=274, Y=212\n",
      "Landmark 18: X=253, Y=176\n",
      "Landmark 19: X=235, Y=156\n",
      "Landmark 20: X=215, Y=140\n",
      "Landmark 0: X=335, Y=331\n",
      "Landmark 1: X=287, Y=335\n",
      "Landmark 2: X=239, Y=316\n",
      "Landmark 3: X=206, Y=297\n",
      "Landmark 4: X=183, Y=280\n",
      "Landmark 5: X=233, Y=249\n",
      "Landmark 6: X=195, Y=236\n",
      "Landmark 7: X=183, Y=247\n",
      "Landmark 8: X=181, Y=257\n",
      "Landmark 9: X=249, Y=227\n",
      "Landmark 10: X=216, Y=203\n",
      "Landmark 11: X=201, Y=198\n",
      "Landmark 12: X=193, Y=194\n",
      "Landmark 13: X=272, Y=215\n",
      "Landmark 14: X=243, Y=180\n",
      "Landmark 15: X=223, Y=160\n",
      "Landmark 16: X=209, Y=144\n",
      "Landmark 17: X=300, Y=211\n",
      "Landmark 18: X=279, Y=174\n",
      "Landmark 19: X=261, Y=152\n",
      "Landmark 20: X=243, Y=133\n",
      "Landmark 0: X=344, Y=325\n",
      "Landmark 1: X=295, Y=338\n",
      "Landmark 2: X=246, Y=321\n",
      "Landmark 3: X=211, Y=299\n",
      "Landmark 4: X=185, Y=279\n",
      "Landmark 5: X=242, Y=246\n",
      "Landmark 6: X=194, Y=239\n",
      "Landmark 7: X=187, Y=264\n",
      "Landmark 8: X=193, Y=284\n",
      "Landmark 9: X=258, Y=225\n",
      "Landmark 10: X=214, Y=216\n",
      "Landmark 11: X=210, Y=237\n",
      "Landmark 12: X=216, Y=251\n",
      "Landmark 13: X=279, Y=213\n",
      "Landmark 14: X=239, Y=186\n",
      "Landmark 15: X=228, Y=186\n",
      "Landmark 16: X=226, Y=185\n",
      "Landmark 17: X=301, Y=210\n",
      "Landmark 18: X=278, Y=170\n",
      "Landmark 19: X=259, Y=149\n",
      "Landmark 20: X=242, Y=129\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    img = cv2.flip(img,1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Extract landmark coordinates\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                h, w, c = img.shape  # Get image dimensions\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                print(f\"Landmark {id}: X={cx}, Y={cy}\")\n",
    "\n",
    "    cv2.imshow(\"MediaPipe Hands\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90a5170d-6355-4d18-b6ed-3e5b924540d7",
   "metadata": {},
   "source": [
    "results = hands.process(img_rgb)\n",
    "\n",
    "Feeds the frame to MediaPipe Hands(We‚Äôre giving (feeding) the frame to the model for analysis.The model then reads (internally) and processes it.)\n",
    "\n",
    "Gets the detection results including multi_hand_landmarks (After MediaPipe processes the image, it returns an object called results ‚Äî and inside this object, there's a property called multi_hand_landmarks which contains the detected landmark points for each hand in the image.If hands are detected ‚Üí this will be a list of hand landmark data. If no hands are detected ‚Üí it will be None)(It just like a list containing the landmark data for each detected hand.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe630f-6337-4cc8-b341-6f24ae36e197",
   "metadata": {},
   "source": [
    "# What this code will do in the upper code\n",
    "if results.multi_hand_landmarks:\n",
    "\n",
    "for hand_landmarks in results.multi_hand_landmarks:\n",
    "  \n",
    "    mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "If any hands are detected:\n",
    "\n",
    "Loops through each detected hand\n",
    "\n",
    "Draws 21 landmark points and connections on the hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae00a9-653c-4381-ae00-2b6ddc06bb77",
   "metadata": {},
   "source": [
    "# Extract Pixel Coordinates\n",
    "# Code\n",
    "for id, lm in enumerate(hand_landmarks.landmark):\n",
    "    h, w, c = img.shape\n",
    "    cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "    print(f\"Landmark {id}: X={cx}, Y={cy}\")\n",
    "# Explaination\n",
    "for id, lm in enumerate(hand_landmarks.landmark):\n",
    "    hand_landmarks.landmark is a list containing 21 landmarks for a hand.\n",
    "    Each lm is one landmark, holding x, y, z values (normalized between 0 to 1).\n",
    "    enumerate() is used so we get:\n",
    "    id ‚Üí index number of landmark (0‚Äì20)\n",
    "    lm ‚Üí the landmark object itself (with x, y, z)\n",
    "h, w, c = img.shape\n",
    "\n",
    "    img.shape gives:\n",
    "    h = image height in pixels\n",
    "    w = image width in pixels\n",
    "    c = number of color channels (should be 3 ‚Üí B, G, R)\n",
    "\n",
    "    Example:\n",
    "\n",
    "    If  webcam frame is 640√ó480 with 3 color channels, then img.shape will be (480, 640, 3)\n",
    "\n",
    "cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "    lm.x and lm.y are normalized values between 0 and 1\n",
    "\n",
    "    ‚Üí 0 = left/top edge, 1 = right/bottom edge (lm.x = 0 ‚Üí point is at the left edge, lm.x = 1 ‚Üí point is at the right edge, lm.x = 0.5 ‚Üí point is in       the middle horizontally, Same thing for lm.y (from top to bottom))\n",
    "\n",
    "    Multiply them by actual image width and height to convert them to pixel positions\n",
    "\n",
    "    int() converts the float to integer because pixel coordinates must be whole numbers.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    If lm.x = 0.5 and image width is 640:\n",
    "\n",
    "    cx = 0.5 * 640 = 320\n",
    "\n",
    "    Same for cy\n",
    "\n",
    "    lm.x and lm.y are between 0 and 1\n",
    "\n",
    "    üëâ This is before multiplying.\n",
    "\n",
    "    When you multiply them by w and h\n",
    "\n",
    "    üëâ You get pixel values\n",
    "\n",
    "    üëâ Pixel values are not between 0 and 1 anymore\n",
    "\n",
    "    üëâ They now range from 0 to image width and 0 to image height\n",
    "\n",
    "    # Why we need pixel value/position:\n",
    "\n",
    "    ‚úÖ Because when we want to interact with the actual image ‚Äî like:\n",
    "\n",
    "    Drawing a circle at a point\n",
    "\n",
    "    Drawing lines between points\n",
    "\n",
    "\n",
    "     We have to use actual pixel positions since images in OpenCV, PIL, or any imaging library are stored as arrays of pixels, indexed by integer values like (x, y) where:\n",
    "\n",
    "    x goes from 0 to image width-1\n",
    "\n",
    "    y goes from 0 to image height-1\n",
    "\n",
    "    üìå So the pixel range is needed for:\n",
    "\n",
    "    Accurately placing/drawing things on the actual image\n",
    "\n",
    "    Interacting with image arrays (since arrays use integer indices)\n",
    "\n",
    "    Measuring actual pixel distances or areas\n",
    "\n",
    "    ‚úÖ Model predicts normalized positions\n",
    "\n",
    "    ‚úÖ We convert them to pixel positions to use on image\n",
    "\n",
    "    ‚úÖ Image pixel values stay unchanged unless we intentionally modify"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0c40b80-e545-4554-81a9-30f4888b11ca",
   "metadata": {},
   "source": [
    "Detects the hand landmarks (like before)\n",
    "\n",
    "Loops through each landmark\n",
    "\n",
    "Converts normalized x, y values to actual pixel positions\n",
    "\n",
    "Prints those positions to your console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f969169-8655-4242-96b6-58d64690b9a6",
   "metadata": {},
   "source": [
    "# Task 2 : Landmark ID Mapping to Hand Anatomy (Mapping)\n",
    "Understand Landmark IDs\n",
    "\n",
    "üìñ Why This Matters:\n",
    "\n",
    "Each hand has 21 landmarks ‚Äî each with a unique ID from 0 to 20.\n",
    "\n",
    "If we know which ID belongs to which part of the hand, we can build logic like:\n",
    "\n",
    "‚ÄúIf landmark 8 is above landmark 6 ‚Üí index finger is up‚Äù\n",
    "\n",
    "‚ÄúIf thumb tip (4) is to the left of thumb MCP (2) ‚Üí thumb is extended‚Äù "
   ]
  },
  {
   "cell_type": "raw",
   "id": "de99a1cf-b188-47fc-a85c-2ad770411c02",
   "metadata": {},
   "source": [
    "üìä MediaPipe Hand Landmark IDs\n",
    "\n",
    "ID\t         Landmark\n",
    "\n",
    "0\t          Wrist\n",
    "1\t          Thumb CMC\n",
    "2\t          Thumb MCP\n",
    "3\t          Thumb IP\n",
    "4\t          Thumb tip\n",
    "5\t          Index finger MCP\n",
    "6\t          Index finger PIP\n",
    "7\t          Index finger DIP\n",
    "8\t          Index finger tip\n",
    "9\t          Middle finger MCP\n",
    "10            Middle finger PIP\n",
    "11\t          Middle finger DIP\n",
    "12\t          Middle finger tip\n",
    "13\t          Ring finger MCP\n",
    "14\t          Ring finger PIP\n",
    "15\t          Ring finger DIP\n",
    "16\t          Ring finger tip\n",
    "17\t          Pinky MCP\n",
    "18\t          Pinky PIP\n",
    "19\t          Pinky DIP\n",
    "20\t          Pinky tip\n",
    "\n",
    "\n",
    "DIP (Distal Interphalangeal joint ‚Äî near fingertip)\n",
    "     This is the joint closest to your fingernail.\n",
    "     It lets the fingertip bend.\n",
    "     Try bending just the top part of your finger ‚Äî that‚Äôs your DIP moving.\n",
    "\n",
    "PIP (Proximal Interphalangeal joint ‚Äî middle joint)\n",
    "     The middle joint of your finger.\n",
    "     Between your knuckle and your fingertip.\n",
    "     It‚Äôs the one that bends when you make a fist or curl your finger halfway.\n",
    "\n",
    "MCP (Metacarpophalangeal joint ‚Äî knuckle)\n",
    "     This is your knuckle joint.\n",
    "     The part where your finger connects to your palm.\n",
    "     If you make a fist, this is the first big bump you see on the back of your hand.\n",
    "\n",
    "CMC (Carpometacarpal joint ‚Äî base of thumb near wrist)\n",
    "     The joint at the very base of your thumb, where it connects to the wrist bones.\n",
    "     It lets your thumb move in multiple directions ‚Äî toward your palm or away.\n",
    "\n",
    "\n",
    "\n",
    "   8   12  16  20    ‚Üê Fingertips\n",
    "   7   11  15  19                                    \n",
    "   6   10  14  18  \n",
    "   5   9   13  17  \n",
    "      0 (Wrist)\n",
    "\n",
    "    Thumb:\n",
    "    1-2-3-4\n",
    "\n",
    "    Index Finger:\n",
    "    5-6-7-8\n",
    "\n",
    "    Middle Finger:\n",
    "    9-10-11-12\n",
    "\n",
    "    Ring Finger:\n",
    "    13-14-15-16\n",
    "\n",
    "    Pinky:\n",
    "    17-18-19-20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b36134-2701-4980-8e1d-24a89082c0a2",
   "metadata": {},
   "source": [
    "# Task 3:  Build Logic to Detect If Finger is Up or Down(How to decide)??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25634abe-00f7-42de-b14a-7be1133e1fd4",
   "metadata": {},
   "source": [
    "üìä Rule: If tip landmark (like 8) is above its lower joint landmark (like 6) in the image üëâ the finger is up.\n",
    "\n",
    "How co-ordinate system works in Opencv??\n",
    "\n",
    "In images and videos (like when we use OpenCV):\n",
    "\n",
    "The origin (0, 0) is at the top-left corner\n",
    "\n",
    "The X-axis increases to the right\n",
    "\n",
    "The Y-axis increases downward\n",
    "\n",
    "so, Smaller y value = higher position , Larger y value = lower position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26412863-5b4d-400d-a3b1-e240ce18b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)  # this takes a value max number of hands as parameter \n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img=cv2.flip(img,1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            # Get landmark positions\n",
    "            h, w, c = img.shape\n",
    "            landmarks = []\n",
    "\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                landmarks.append((id, cx, cy))\n",
    "\n",
    "            \n",
    "            # Now check if Index Finger is up\n",
    "            index_tip_y = landmarks[8][2]\n",
    "            index_pip_y = landmarks[6][2]\n",
    "\n",
    "            if index_tip_y < index_pip_y:\n",
    "                cv2.putText(img, \"Index Finger UP\", (10, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(img, \"Index Finger DOWN\", (10, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"Hand Tracking\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79637c-8c01-49da-8d6f-3539176315c7",
   "metadata": {},
   "source": [
    "# Explaination of the code\n",
    "h, w, c = img.shape\n",
    "landmarks = []\n",
    "for id, lm in enumerate(hand_landmarks.landmark):\n",
    "cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "landmarks.append((id, cx, cy))\n",
    "\n",
    "    In this img.shape is used to get the dimensions of the img which is used in normalization\n",
    "    after then we have created a list landmark here we are storing all the landmark information for each frame.\n",
    "    The landmarks array holds tuples of (ID, X, Y) for each landmark. \n",
    "    Here:\n",
    "    id is the landmark ID (like 0 for wrist, 8 for index finger tip, etc.).\n",
    "    cx is the X position of the landmark in pixels.\n",
    "    cy is the Y position of the landmark in pixels.\n",
    "\n",
    "\n",
    "index_tip_y = landmarks[8][2]\n",
    "index_pip_y = landmarks[6][2]\n",
    "\n",
    "    The landmarks list allows you to access the position of any landmark by its ID (e.g., landmarks[8] for the index finger tip).\n",
    "    üìå Why landmarks[8][2]? What is the use of 2 here??\n",
    "    Now ‚Äî Python list/tuple indexing works like this:\n",
    "    [0] ‚Üí first value\n",
    "    [1] ‚Üí second value\n",
    "    [2] ‚Üí third value\n",
    "    Meaning:\n",
    "    landmarks[8][0] ‚Üí 8 (the landmark ID)\n",
    "    landmarks[8][1] ‚Üí cx (X position)\n",
    "    landmarks[8][2] ‚Üí cy (Y position)\n",
    "\n",
    "    Here We‚Äôre grabbing the Y coordinate (vertical position in pixels) of the index finger tip.\n",
    "\n",
    "    üìñ Why do we need Y position?\n",
    "    Because ‚Äî to check if a finger is up, you need to see if the tip‚Äôs Y position is higher (i.e. smaller) than the middle joint‚Äôs Y position.\n",
    "\n",
    "if index_tip_y < index_pip_y:\n",
    "    cv2.putText(img, \"Index Finger UP\", (10, 50),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    " else:\n",
    "    cv2.putText(img, \"Index Finger DOWN\", (10, 50),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    This checks whether the tip of the index finger is above its proximal interphalangeal (PIP) joint in the image.\n",
    "    If up then Then draws the text \"Index Finger UP\" on the image at coordinates (10, 50). Uses the FONT_HERSHEY_SIMPLEX font. Text size scale is 1.\n",
    "    Color is (0, 255, 0) ‚Üí green. Thickness is 2 pixels.\n",
    "    If the tip is not above the PIP joint (i.e., it‚Äôs at same level or lower down the image), then. Draws \"Index Finger DOWN\" text at the same position.\n",
    "    Color is (0, 0, 255) ‚Üí red. Thickness is 2 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b46238-073f-49ec-bcee-60f417d193e0",
   "metadata": {},
   "source": [
    "# Task 4 :  Detect and track both hands if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21729eca-d40b-4fb9-aa8a-b8c53ca6ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2)  # Allow up to 2 hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img,1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_no, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Draw landmarks\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get image dimensions\n",
    "            h, w, c = img.shape\n",
    "\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                if id == 8:  # Index Finger Tip\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 255), cv2.FILLED)\n",
    "                    cv2.putText(img, f\"Hand {hand_no+1} - Index Tip\", (cx-50, cy-20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Multiple Hands\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efb23e2b-7e75-4025-a90a-658f406d8c19",
   "metadata": {},
   "source": [
    "if id == 8:  # Index Finger Tip\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 255), cv2.FILLED)\n",
    "                    cv2.putText(img, f\"Hand {hand_no+1} - Index Tip\", (cx-50, cy-20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "\n",
    "\n",
    "In MediaPipe Hands, each landmark on a hand has a fixed ID ‚Äî and ID 8 corresponds to the tip of the index finger.\n",
    "\n",
    "This draws a filled circle on the img at position (cx, cy).\n",
    "\n",
    "(cx, cy) would be the x and y coordinates of the landmark with id == 8.\n",
    "\n",
    "5 is the radius of the circle.\n",
    "\n",
    "(255, 0, 255) is the color in BGR format ‚Äî this is a magenta/pink color.\n",
    "\n",
    "cv2.FILLED means the circle will be completely filled.\n",
    "\n",
    "This adds a text label to the image img.\n",
    "\n",
    "f\"Hand {hand_no+1} - Index Tip\" is the text that will be displayed, showing which hand number this is (since hand_no likely starts from 0, adding 1 makes it more readable ‚Äî like \"Hand 1\" or \"Hand 2\") and noting it‚Äôs the Index Tip.\n",
    "\n",
    "(cx-50, cy-20) is the position where the text starts ‚Äî slightly above and to the left of the index tip circle for visibility.\n",
    "\n",
    "cv2.FONT_HERSHEY_SIMPLEX is the font type.\n",
    "\n",
    "0.5 is the font scale.\n",
    "\n",
    "(0,255,0) is the text color in BGR ‚Äî which is green.\n",
    "\n",
    "2 is the thickness of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a07e3-faf0-4beb-96c0-23db2fe00ee9",
   "metadata": {},
   "source": [
    "# Task 5: Count Number of Fingers Up in right hand"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cb3cd50-1c6a-4a18-b50c-c9e0e5c389e1",
   "metadata": {},
   "source": [
    "se MediaPipe Hand landmarks\n",
    "\n",
    "Identify tip landmarks for fingers (e.g. index tip = 8)\n",
    "\n",
    "Compare tip y/x position with its corresponding lower joint\n",
    "(if tip is above the joint, the finger is considered 'up')\n",
    "\n",
    "Count the total number of fingers up\n",
    "\n",
    "Display the count as text on the video frame"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37ae8169-4541-4be8-a98a-3dac5ab4c65c",
   "metadata": {},
   "source": [
    "üñêÔ∏è Landmark Indexes for Fingers:\n",
    "Finger\t   Tip Landmark\t      Lower Joint\n",
    "Thumb\t      4\t                    3\n",
    "Index\t      8\t                    6\n",
    "Middle        12                  \t10\n",
    "Ring\t      16\t                14\n",
    "Little\t      20\t                18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ceb22-b3d6-49c2-8e16-c5f462bc427b",
   "metadata": {},
   "source": [
    "Since in mediapipe there 21 land marks and while receiving our hand we receive all these landmarks \n",
    "\n",
    "So instead of checking all 21 landmarks every time, I just provided the list of the finger tips:\n",
    "\n",
    "These are always the tips, and we use them to:\n",
    "\n",
    "Count how many fingers are raised.\n",
    "\n",
    "Check gesture conditions.\n",
    "\n",
    "Do finger detection logic like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4747cf02-4813-4745-a146-2431c7e42123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# MediaPipe hands module setup\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2)  # Now detect up to 2 hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Landmark indices for finger tips\n",
    "finger_tips_ids = [4, 8, 12, 16, 20]\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img,1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    total_count = 0  # This will hold the sum of fingers up from all detected hands.\n",
    "\n",
    "    if results.multi_hand_landmarks:      #If hands are detected, loop over each detected hand.\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            lm_list = []\n",
    "            h, w, c = img.shape\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                lm_list.append((int(lm.x * w), int(lm.y * h)))        #Converts MediaPipe‚Äôs normalized landmark positions (0-1) \n",
    "                                                         #to pixel coordinates based on image width and height to get Pixel Positions of Landmarks\n",
    " \n",
    "            fingers = []\n",
    "\n",
    "            # Thumb (check x-axis because it's sideways)\n",
    "            if lm_list[finger_tips_ids[0]][0] < lm_list[finger_tips_ids[0] - 1][0]:\n",
    "                fingers.append(1)\n",
    "            else:\n",
    "                fingers.append(0)\n",
    "\n",
    "            # Other four fingers (check y-axis)\n",
    "            for id in range(1, 5):\n",
    "                if lm_list[finger_tips_ids[id]][1] < lm_list[finger_tips_ids[id] - 2][1]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "\n",
    "            total_count += fingers.count(1) #Counts how many 1s are in the fingers list (fingers up)\n",
    "                                             #and adds to the overall total_count.\n",
    "\n",
    "            # Draw landmarks and connections\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the total count on screen\n",
    "    cv2.rectangle(img, (20, 300), (270, 425), (0, 255, 0), cv2.FILLED) #Draws a green rectangle\n",
    "    cv2.putText(img, str(total_count), (45, 400), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                5, (255, 0, 0  ), 10) #Prints the current total finger count in large blue text.\n",
    "\n",
    "    cv2.imshow(\"Finger Counter\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a1d2e-6017-4784-b1b4-4f28b2b0e72a",
   "metadata": {},
   "source": [
    "# ü§î Why do we create the fingers list?\n",
    "we want to store the state (up/down) of each of the 5 fingers ‚Äî one by one ‚Äî for the current hand we're analyzing.\n",
    "\n",
    "Each value in this list will be either:\n",
    "\n",
    "1 ‚Üí finger is up ‚úÖ\n",
    "\n",
    "0 ‚Üí finger is down ‚ùå\n",
    "\n",
    "After calculating which fingers are up (based on tip position vs joint), we append the result (0 or 1) to this list."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc64f8b0-2f2e-4014-944f-d50def0cf531",
   "metadata": {},
   "source": [
    " if lm_list[finger_tips_ids[0]][0] > lm_list[finger_tips_ids[0] - 1][0]:\n",
    "                fingers.append(1)\n",
    "            else:\n",
    "                fingers.append(0)\n",
    "\n",
    "\n",
    "This is checking whether the thumb is open or closed.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "finger_tips_ids[0] ‚Üí this is 4, the landmark index for the tip of the thumb.\n",
    "\n",
    "finger_tips_ids[0] - 1 ‚Üí this is 3, the landmark just before the thumb tip (which is towards the palm).\n",
    "\n",
    "Then:\n",
    "\n",
    "lm_list[finger_tips_ids[0]][0] ‚Üí gets the x-coordinate of the thumb tip.\n",
    "\n",
    "lm_list[finger_tips_ids[0] - 1][0] ‚Üí gets the x-coordinate of the landmark next to the thumb tip.\n",
    "\n",
    "Then:\n",
    "\n",
    "If the thumb tip‚Äôs x-position is greater than the x-position of the joint just before it, it means the thumb is pointing to the right (in a right-hand image), hence it's open.\n",
    "\n",
    "So:\n",
    "\n",
    "fingers.append(1) ‚Üí appends 1 (meaning \"thumb is up/open\").\n",
    "\n",
    "else ‚Üí appends 0 (meaning \"thumb is down/closed\").\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c44bb7e6-9b03-46fa-b2e3-e69613aa2eed",
   "metadata": {},
   "source": [
    "for id in range(1, 5):\n",
    "    if lm_list[finger_tips_ids[id]][1] < lm_list[finger_tips_ids[id] - 2][1]:\n",
    "        fingers.append(1)\n",
    "    else:\n",
    "        fingers.append(0)\n",
    "\n",
    "\n",
    "lm_list ‚Üí list of all hand landmarks detected (with [x, y] or [x, y, z] positions)\n",
    "\n",
    "finger_tips_ids ‚Üí list of landmark indices for the finger tips\n",
    "Typically: [4, 8, 12, 16, 20] (Thumb, Index, Middle, Ring, Pinky)\n",
    "\n",
    "id in range(1, 5) ‚Üí loops through Index (8), Middle (12), Ring (16), Pinky (20)\n",
    "\n",
    "\n",
    "üîç Inside the loop:\n",
    "finger_tips_ids[id] gives the landmark index for the current finger tip.\n",
    "\n",
    "e.g. for id = 1 ‚Üí finger_tips_ids[1] = 8 (Index fingertip)\n",
    "\n",
    "finger_tips_ids[id] - 2 gives the landmark two points below the tip on the same finger.\n",
    "\n",
    "e.g. 8 - 2 = 6 ‚Üí this is typically the middle joint landmark for the Index finger.\n",
    "\n",
    "Then:\n",
    "\n",
    "lm_list[finger_tips_ids[id]][1] ‚Üí the y-coordinate of the fingertip\n",
    "\n",
    "lm_list[finger_tips_ids[id] - 2][1] ‚Üí the y-coordinate of the joint two points below.\n",
    "\n",
    "if lm_list[finger_tips_ids[id]][1] < lm_list[finger_tips_ids[id] - 2][1]:\n",
    "In images, y-axis increases downward (top = 0, bottom = bigger value)\n",
    "\n",
    "So, if the fingertip's y-position is less than the joint‚Äôs y-position, it means the fingertip is above the joint ‚Üí the finger is extended.\n",
    "\n",
    "üëâ If true: append 1 (finger is up)\n",
    "üëâ Else: append 0 (finger is down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02f93215-3f65-4972-b216-a56c95011912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2)  # Allow up to 2 hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_no, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Draw landmarks\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get image dimensions\n",
    "            h, w, c = img.shape\n",
    "\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                if id == 8:  # Index Finger Tip\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 255), cv2.FILLED)\n",
    "                    cv2.putText(img, f\"Hand {hand_no+1} - Index Tip\", (cx-50, cy-20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Multiple Hands\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc985eb-0197-4dee-a190-2b02b129703b",
   "metadata": {},
   "source": [
    "# Task 6 : Updated code of task 5 with left/right detection¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "981e9a43-a62f-465d-b087-e3ec7be0c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "finger_tips_ids = [4, 8, 12, 16, 20]\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img,1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    total_count = 0\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_index, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            lm_list = []\n",
    "            h, w, c = img.shape\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                lm_list.append((int(lm.x * w), int(lm.y * h)))\n",
    "\n",
    "            fingers = []\n",
    "\n",
    "            # Get handedness: \"Right\" or \"Left\"\n",
    "            hand_label = results.multi_handedness[hand_index].classification[0].label\n",
    "\n",
    "            # Thumb logic depends on hand\n",
    "            if hand_label == \"Right\":\n",
    "                if lm_list[finger_tips_ids[0]][0] < lm_list[finger_tips_ids[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "            else:  # Left hand\n",
    "                if lm_list[finger_tips_ids[0]][0] > lm_list[finger_tips_ids[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "\n",
    "            # Other four fingers (common for both hands)\n",
    "            for id in range(1, 5):\n",
    "                if lm_list[finger_tips_ids[id]][1] < lm_list[finger_tips_ids[id] - 2][1]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "\n",
    "            total_count += fingers.count(1)\n",
    "\n",
    "            # Draw landmarks\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display total count\n",
    "    cv2.rectangle(img, (20, 300), (270, 425), (0, 255, 0), cv2.FILLED)\n",
    "    cv2.putText(img, str(total_count), (45, 400), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                5, (255, 0, 0), 10)\n",
    "\n",
    "    cv2.imshow(\"Finger Counter\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f497376-1390-4aae-983f-0adb982b786c",
   "metadata": {},
   "source": [
    "hand_label = results.multi_handedness[hand_index].classification[0].label\n",
    "\n",
    "    results.multi_handedness:- gives you a list of detected hands‚Äô handedness (i.e. whether    it's a left or right hand).\n",
    "\n",
    "    hand_index:- points to a specific hand from the detected list (like the first or second hand detected in the frame).\n",
    "\n",
    "    classification[0].label:- accesses the classification result for that hand ‚Äî and this will be either \"Left\" or \"Right\".\n",
    "\n",
    "MediaPipe uses the relative position of landmarks in the detected hand region to infer whether it's a left or right hand, based on its internal trained model. It looks at landmark orientations and positions relative to each other (like thumb vs. index finger positions) to guess handedness.\n",
    "\n",
    "\n",
    "\n",
    "# Detailed \n",
    "\n",
    "1.Hand Detection\n",
    "    First, MediaPipe‚Äôs palm detection model finds potential hand regions in the image (it's a lightweight detector designed for speed).\n",
    "\n",
    "2.Hand Landmark Model\n",
    "    Once a hand is detected, another model runs inside that region to identify 21 hand landmarks (key points like fingertips, joints, wrist etc.) ‚Äî in 3D (x, y, z coordinates).\n",
    "    0: Wrist\n",
    "    1-4: Thumb points\n",
    "    5-8: Index finger points\n",
    "    9-12: Middle finger points\n",
    "    13-16: Ring finger points\n",
    "    17-20: Pinky finger points\n",
    "\n",
    "3.Handedness Classification\n",
    "    After getting the 21 landmarks, MediaPipe uses the positions and orientations of these landmarks to classify whether it‚Äôs a left or right hand.\n",
    "\n",
    "\n",
    "üìè How Does It Decide Left or Right?\n",
    "\n",
    "The model essentially looks at the relative positions of key landmarks like the thumb tip (landmark 4) and index finger (landmark 8).\n",
    "\n",
    "Based on the spatial arrangement and orientation of these points (like the thumb being on the left or right side of the hand's palm region, relative to the index and middle fingers), it can infer the handedness.\n",
    "\n",
    "For example:\n",
    "\n",
    "If thumb landmark is on the left side of the detected hand region ‚Üí it‚Äôs likely a Right hand.\n",
    "\n",
    "If thumb landmark is on the right side of the detected hand region ‚Üí it‚Äôs likely a Left hand.\n",
    "\n",
    "This is done internally by a trained classification model that has learned to recognize these patterns from labeled training images.\n",
    "\n",
    "\n",
    "Along with the label ('Left' or 'Right'), MediaPipe also provides a confidence score ‚Äî how sure the model is about its decision.\n",
    "\n",
    "results.multi_handedness[hand_index].classification[0].score\n",
    " Example: 0.95 (95% confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bdb43c-c70a-42af-a44e-1b4272e4fdd5",
   "metadata": {},
   "source": [
    "# Task 7: Measure Distance Between Two Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb54bb4a-b87f-4426-ba50-39639e229b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 138\n",
      "Distance: 118\n",
      "Distance: 125\n",
      "Distance: 126\n",
      "Distance: 129\n",
      "Distance: 131\n",
      "Distance: 134\n",
      "Distance: 129\n",
      "Distance: 128\n",
      "Distance: 36\n",
      "Distance: 39\n",
      "Distance: 45\n",
      "Distance: 36\n",
      "Distance: 33\n",
      "Distance: 33\n",
      "Distance: 34\n",
      "Distance: 31\n",
      "Distance: 33\n",
      "Distance: 39\n",
      "Distance: 41\n",
      "Distance: 58\n",
      "Distance: 64\n",
      "Distance: 116\n",
      "Distance: 129\n",
      "Distance: 132\n",
      "Distance: 143\n",
      "Distance: 141\n",
      "Distance: 141\n",
      "Distance: 141\n",
      "Distance: 140\n",
      "Distance: 138\n",
      "Distance: 139\n",
      "Distance: 127\n",
      "Distance: 124\n",
      "Distance: 125\n",
      "Distance: 123\n",
      "Distance: 123\n",
      "Distance: 123\n",
      "Distance: 123\n",
      "Distance: 129\n",
      "Distance: 130\n",
      "Distance: 131\n",
      "Distance: 125\n",
      "Distance: 125\n",
      "Distance: 126\n",
      "Distance: 126\n",
      "Distance: 127\n",
      "Distance: 129\n",
      "Distance: 128\n",
      "Distance: 130\n",
      "Distance: 123\n",
      "Distance: 122\n",
      "Distance: 123\n",
      "Distance: 102\n",
      "Distance: 25\n",
      "Distance: 15\n",
      "Distance: 17\n",
      "Distance: 15\n",
      "Distance: 14\n",
      "Distance: 15\n",
      "Distance: 12\n",
      "Distance: 14\n",
      "Distance: 10\n",
      "Distance: 14\n",
      "Distance: 23\n",
      "Distance: 69\n",
      "Distance: 105\n",
      "Distance: 109\n",
      "Distance: 164\n",
      "Distance: 176\n",
      "Distance: 176\n",
      "Distance: 177\n",
      "Distance: 186\n",
      "Distance: 199\n",
      "Distance: 207\n",
      "Distance: 207\n",
      "Distance: 211\n",
      "Distance: 216\n",
      "Distance: 217\n",
      "Distance: 217\n",
      "Distance: 219\n",
      "Distance: 219\n",
      "Distance: 220\n",
      "Distance: 205\n",
      "Distance: 206\n",
      "Distance: 47\n",
      "Distance: 21\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img,1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get height, width\n",
    "            h, w, c = img.shape\n",
    "\n",
    "            # Get landmark positions\n",
    "            x1, y1 = int(hand_landmarks.landmark[4].x * w), int(hand_landmarks.landmark[4].y * h)\n",
    "            x2, y2 = int(hand_landmarks.landmark[8].x * w), int(hand_landmarks.landmark[8].y * h)\n",
    "\n",
    "            # Draw circles at points\n",
    "            cv2.circle(img, (x1, y1), 10, (255, 0, 0), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 10, (255, 0, 0), cv2.FILLED)\n",
    "\n",
    "            # Draws a green line connecting both tips.\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "            # Calculate distance\n",
    "            distance = int(math.hypot(x2 - x1, y2 - y1))\n",
    "            print(\"Distance:\", distance)\n",
    "\n",
    "            # Show distance on screen\n",
    "            cv2.putText(img, f'{distance}', ((x1 + x2)//2, (y1 + y2)//2),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Hand Tracking\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1844e15-deb9-4da0-8976-9be31f6f1c18",
   "metadata": {},
   "source": [
    "x1, y1 = int(hand_landmarks.landmark[4].x * w), int(hand_landmarks.landmark[4].y * h)\n",
    "x2, y2 = int(hand_landmarks.landmark[8].x * w), int(hand_landmarks.landmark[8].y * h)\n",
    "\n",
    "(x1, y1) ‚Üí pixel position of thumb tip\n",
    "\n",
    "(x2, y2) ‚Üí pixel position of index finger tip\n",
    "\n",
    "    hand_landmarks.landmark[4] ‚Üí refers to landmark number 4, which is the tip of the thumb in the MediaPipe hand model.\n",
    "\n",
    "    hand_landmarks.landmark[8] ‚Üí refers to landmark number 8, which is the tip of the index finger.\n",
    "\n",
    "    üìå Why multiply by w and h?\n",
    "MediaPipe gives normalized coordinates for each landmark:\n",
    "\n",
    "landmark.x and landmark.y are values between 0 and 1\n",
    "(relative to the image width and height)\n",
    "\n",
    "To convert these to actual pixel positions on the image:\n",
    "\n",
    "Multiply landmark.x by image width w\n",
    "\n",
    "Multiply landmark.y by image height h\n",
    "\n",
    "And convert to integer (since pixel positions are whole numbers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1c67ccf-add8-4053-a1c7-9cd650551480",
   "metadata": {},
   "source": [
    "distance = int(math.hypot(x2 - x1, y2 - y1))\n",
    "\n",
    "math.hypot() is a function from Python‚Äôs math module that calculates the Euclidean distance (the straight-line distance) from the origin (0,0) to a point (x, y) in a 2D space, or between any two points‚Äô difference values (dx, dy).\n",
    "\n",
    "It uses the Pythagorean theorem:\n",
    "It takes coordinate difference as input as same in distance formula\n",
    "\n",
    "\n",
    "üëâ It calculates the Euclidean distance between two points:\n",
    "(x1, y1) ‚Üí thumb tip\n",
    "(x2, y2) ‚Üí index fingertip\n",
    "\n",
    "using pythagorean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc7310-b681-4c44-a3fd-0e61f1cfbd15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MediaPipe)",
   "language": "python",
   "name": "mediapipe-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
